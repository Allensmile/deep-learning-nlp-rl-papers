
Table of Contents
=================

  * [Articles](#articles)
    * [2017\-01](#2017-01)
      * [A Simple and Accurate Syntax\-Agnostic Neural Model for Dependency\-based Semantic Role Labeling](#a-simple-and-accurate-syntax-agnostic-neural-model-for-dependency-based-semantic-role-labeling)
      * [Reinforcement Learning via Recurrent Convolutional Neural Networks](#reinforcement-learning-via-recurrent-convolutional-neural-networks)
      * [Visualizing Residual Networks](#visualizing-residual-networks)
      * [Multi\-level Representations for Fine\-Grained Typing of Knowledge Base Entities](#multi-level-representations-for-fine-grained-typing-of-knowledge-base-entities)
      * [Neural Personalized Response Generation as Domain Adaptation](#neural-personalized-response-generation-as-domain-adaptation)
      * [Task\-Specific Attentive Pooling of Phrase Alignments Contributes to Sentence Matching](#task-specific-attentive-pooling-of-phrase-alignments-contributes-to-sentence-matching)
      * [Structural Attention Neural Networks for improved sentiment analysis](#structural-attention-neural-networks-for-improved-sentiment-analysis)
      * [Self\-Taught Convolutional Neural Networks for Short Text Clustering](#self-taught-convolutional-neural-networks-for-short-text-clustering)
      * [Textual Entailment with Structured Attentions and Composition](#textual-entailment-with-structured-attentions-and-composition)
      * [Unsupervised neural and Bayesian models for zero\-resource speech processing](#unsupervised-neural-and-bayesian-models-for-zero-resource-speech-processing)
      * [NIPS 2016 Tutorial: Generative Adversarial Networks](#nips-2016-tutorial-generative-adversarial-networks)
      * [Dense Associative Memory is Robust to Adversarial Inputs](#dense-associative-memory-is-robust-to-adversarial-inputs)
      * [A K\-fold Method for Baseline Estimation in Policy Gradient Algorithms](#a-k-fold-method-for-baseline-estimation-in-policy-gradient-algorithms)
      * [Generating Long and Diverse Responses with Neural Conversation Models](#generating-long-and-diverse-responses-with-neural-conversation-models)
      * [Simplified Gating in Long Short\-term Memory (LSTM) Recurrent Neural Networks](#simplified-gating-in-long-short-term-memory-lstm-recurrent-neural-networks)
      * [Modularized Morphing of Neural Networks](#modularized-morphing-of-neural-networks)
      * [A Copy\-Augmented Sequence\-to\-Sequence Architecture Gives Good Performance on Task\-Oriented Dialogue](#a-copy-augmented-sequence-to-sequence-architecture-gives-good-performance-on-task-oriented-dialogue)
      * [Dialog Context Language Modeling with Recurrent Neural Networks](#dialog-context-language-modeling-with-recurrent-neural-networks)
      * [Neural Models for Sequence Chunking](#neural-models-for-sequence-chunking)
      * [DyNet: The Dynamic Neural Network Toolkit](#dynet-the-dynamic-neural-network-toolkit)
      * [Understanding the Effective Receptive Field in Deep Convolutional Neural Networks](#understanding-the-effective-receptive-field-in-deep-convolutional-neural-networks)
      * [Agent\-Agnostic Human\-in\-the\-Loop Reinforcement Learning](#agent-agnostic-human-in-the-loop-reinforcement-learning)
      * [Minimally Naturalistic Artificial Intelligence](#minimally-naturalistic-artificial-intelligence)
      * [Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks](#adversarial-variational-bayes-unifying-variational-autoencoders-and-generative-adversarial-networks)
      * [Adversarial Learning for Neural Dialogue Generation](#adversarial-learning-for-neural-dialogue-generation)
      * [A Multichannel Convolutional Neural Network For Cross\-language Dialog State Tracking](#a-multichannel-convolutional-neural-network-for-cross-language-dialog-state-tracking)
      * [Learning to Decode for Future Success](#learning-to-decode-for-future-success)
      * [Outrageously Large Neural Networks: The Sparsely\-Gated Mixture\-of\-Experts Layer](#outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layer)
      * [Regularizing Neural Networks by Penalizing Confident Output Distributions](#regularizing-neural-networks-by-penalizing-confident-output-distributions)
      * [Discriminative Neural Topic Models](#discriminative-neural-topic-models)
      * [Hierarchical Recurrent Attention Network for Response Generation](#hierarchical-recurrent-attention-network-for-response-generation)
      * [Wasserstein GAN](#wasserstein-gan)
      * [Reinforced backpropagation improves test performance of deep networks: a toy\-model study](#reinforced-backpropagation-improves-test-performance-of-deep-networks-a-toy-model-study)
      * [CommAI: Evaluating the first steps towards a useful general AI](#commai-evaluating-the-first-steps-towards-a-useful-general-ai)
    * [2017\-02](#2017-02)
      * [Low\-Dose CT with a Residual Encoder\-Decoder Convolutional Neural Network (RED\-CNN)](#low-dose-ct-with-a-residual-encoder-decoder-convolutional-neural-network-red-cnn)
      * [On orthogonality and learning recurrent networks with long term dependencies](#on-orthogonality-and-learning-recurrent-networks-with-long-term-dependencies)
      * [Understanding trained CNNs by indexing neuron selectivity](#understanding-trained-cnns-by-indexing-neuron-selectivity)
      * [Design, Analysis and Application of A Volumetric Convolutional Neural Network](#design-analysis-and-application-of-a-volumetric-convolutional-neural-network)
      * [On SGD's Failure in Practice: Characterizing and Overcoming Stalling](#on-sgds-failure-in-practice-characterizing-and-overcoming-stalling)
      * [Symbolic, Distributed and Distributional Representations for Natural Language Processing in the Era of Deep Learning: a Survey](#symbolic-distributed-and-distributional-representations-for-natural-language-processing-in-the-era-of-deep-learning-a-survey)
      * [Multilingual Multi\-modal Embeddings for Natural Language Processing](#multilingual-multi-modal-embeddings-for-natural-language-processing)
      * [Doubly\-Attentive Decoder for Multi\-modal Neural Machine Translation](#doubly-attentive-decoder-for-multi-modal-neural-machine-translation)
      * [Opinion Recommendation using Neural Memory Model](#opinion-recommendation-using-neural-memory-model)
      * [Search Intelligence: Deep Learning For Dominant Category Prediction](#search-intelligence-deep-learning-for-dominant-category-prediction)
      * [Syntax\-aware Neural Machine Translation Using CCG](#syntax-aware-neural-machine-translation-using-ccg)
      * [Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks](#reluplex-an-efficient-smt-solver-for-verifying-deep-neural-networks)
      * [Neural Semantic Parsing over Multiple Knowledge\-bases](#neural-semantic-parsing-over-multiple-knowledge-bases)
      * [All\-but\-the\-Top: Simple and Effective Postprocessing for Word Representations](#all-but-the-top-simple-and-effective-postprocessing-for-word-representations)
      * [Deep Learning With Denamic Computation Graph](#deep-learning-with-denamic-computation-graph)
      * [A Knowledge\-Grounded Neural Conversation Model](#a-knowledge-grounded-neural-conversation-model)
      * [Comparative Study of CNN and RNN for Natural Language Processing](#comparative-study-of-cnn-and-rnn-for-natural-language-processing)
      * [Neural Discourse Structure for Text Categorization](#neural-discourse-structure-for-text-categorization)
      * [Living a discrete life in a continuous world: Reference with distributed representations](#living-a-discrete-life-in-a-continuous-world-reference-with-distributed-representations)
      * [Beam Search Strategies for Neural Machine Translation](#beam-search-strategies-for-neural-machine-translation)
      * [Ensemble Distillation for Neural Machine Translation](#ensemble-distillation-for-neural-machine-translation)
      * [Multi\-task Coupled Attentions for Category\-specific Aspect and Opinion Terms Co\-extraction](#multi-task-coupled-attentions-for-category-specific-aspect-and-opinion-terms-co-extraction)
      * [Fast and Accurate Sequence Labeling with Iterated Dilated Convolutions](#fast-and-accurate-sequence-labeling-with-iterated-dilated-convolutions)
      * [Learning similarity preserving representations with neural similarity encoders](#learning-similarity-preserving-representations-with-neural-similarity-encoders)
      * [Semi\-Supervised QA with Generative Domain\-Adaptive Nets](#semi-supervised-qa-with-generative-domain-adaptive-nets)
      * [Trainable Greedy Decoding for Neural Machine Translation](#trainable-greedy-decoding-for-neural-machine-translation)
      * [Automatic Rule Extraction from Long Short Term Memory Networks](#automatic-rule-extraction-from-long-short-term-memory-networks)
      * [A Hybrid Convolutional Variational Autoencoder for Text Generation](#a-hybrid-convolutional-variational-autoencoder-for-text-generation)
      * [Iterative Multi\-document Neural Attention for Multiple Answer Prediction](#iterative-multi-document-neural-attention-for-multiple-answer-prediction)
      * [Question Answering through Transfer Learning from Large Fine\-grained Supervision Data](#question-answering-through-transfer-learning-from-large-fine-grained-supervision-data)
      * [Neural Machine Translation with Source\-Side Latent Graph Parsing](#neural-machine-translation-with-source-side-latent-graph-parsing)
      * [Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization](#exploiting-domain-knowledge-via-grouped-weight-sharing-with-application-to-text-categorization)
      * [How to evaluate word embeddings? On importance of data efficiency and simple supervised tasks](#how-to-evaluate-word-embeddings-on-importance-of-data-efficiency-and-simple-supervised-tasks)
      * [Character\-level Deep Conflation for Business Data Analytics](#character-level-deep-conflation-for-business-data-analytics)
      * [Convolutional Neural Network for Humor Recognition](#convolutional-neural-network-for-humor-recognition)
      * [Parallel Long Short\-Term Memory for Multi\-stream Classification](#parallel-long-short-term-memory-for-multi-stream-classification)
      * [Batch Policy Gradient Methods for Improving Neural Conversation Models](#batch-policy-gradient-methods-for-improving-neural-conversation-models)
      * [Offline bilingual word vectors, orthogonal transformations and the inverted softmax](#offline-bilingual-word-vectors-orthogonal-transformations-and-the-inverted-softmax)
      * [A Morphology\-aware Network for Morphological Disambiguation](#a-morphology-aware-network-for-morphological-disambiguation)
      * [Learning to Parse and Translate Improves Neural Machine Translation](#learning-to-parse-and-translate-improves-neural-machine-translation)
      * [Exploring loss function topology with cyclical learning rates](#exploring-loss-function-topology-with-cyclical-learning-rates)
      * [Frustratingly Short Attention Spans in Neural Language Modeling](#frustratingly-short-attention-spans-in-neural-language-modeling)
      * [A Dependency\-Based Neural Reordering Model for Statistical Machine Translation](#a-dependency-based-neural-reordering-model-for-statistical-machine-translation)
      * [Training Language Models Using Target\-Propagation](#training-language-models-using-target-propagation)
      * [Latent Variable Dialogue Models and their Diversity](#latent-variable-dialogue-models-and-their-diversity)
      * [Reproducing and learning new algebraic operations on word embeddings using genetic programming](#reproducing-and-learning-new-algebraic-operations-on-word-embeddings-using-genetic-programming)
      * [soc2seq: Social Embedding meets Conversation Model](#soc2seq-social-embedding-meets-conversation-model)
      * [An Attention\-Based Deep Net for Learning to Rank](#an-attention-based-deep-net-for-learning-to-rank)
      * [Collaborative Deep Reinforcement Learning](#collaborative-deep-reinforcement-learning)
      * [Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning](#learning-to-repeat-fine-grained-action-repetition-for-deep-reinforcement-learning)
      * [Collaborative Deep Reinforcement Learning for Joint Object Search](#collaborative-deep-reinforcement-learning-for-joint-object-search)
      * [Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks](#cosine-normalization-using-cosine-similarity-instead-of-dot-product-in-neural-networks)
      * [On Loss Functions for Deep Neural Networks in Classification](#on-loss-functions-for-deep-neural-networks-in-classification)
      * [Dataset Augmentation in Feature Space](#dataset-augmentation-in-feature-space)
      * [An Extended Framework for Marginalized Domain Adaptation](#an-extended-framework-for-marginalized-domain-adaptation)
      * [Revisiting Perceptron: Efficient and Label\-Optimal Active Learning of Halfspaces](#revisiting-perceptron-efficient-and-label-optimal-active-learning-of-halfspaces)
      * [Distributed Second\-Order Optimization Using Kronecker\-Factored Approximations](#distributed-second-order-optimization-using-kronecker-factored-approximations)
      * [Mimicking Ensemble Learning with Deep Branched Networks](#mimicking-ensemble-learning-with-deep-branched-networks)
      * [Style Transfer Generative Adversarial Networks: Learning to Play Chess Differently](#style-transfer-generative-adversarial-networks-learning-to-play-chess-differently)
      * [Learning to Draw Dynamic Agent Goals with Generative Adversarial Networks](#learning-to-draw-dynamic-agent-goals-with-generative-adversarial-networks)
      * [Data Distillation for Controlling Specificity in Dialogue Generation](#data-distillation-for-controlling-specificity-in-dialogue-generation)
      * [Context\-Aware Prediction of Derivational Word\-forms](#context-aware-prediction-of-derivational-word-forms)
      * [Tackling Error Propagation through Reinforcement Learning: A Case of Greedy Dependency Parsing](#tackling-error-propagation-through-reinforcement-learning-a-case-of-greedy-dependency-parsing)
      * [Active One\-shot Learning](#active-one-shot-learning)

Articles
========
## 2017-01
### A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling

**Authors:** Diego Marcheggiani, Anton Frolov, Ivan Titov

**Abstract:** We introduce a simple and accurate neural model for dependency-based semantic role labeling. Our model predicts predicate-argument dependencies relying on states of a bidirectional LSTM encoder. The semantic role labeler achieves respectable performance on English even without any kind of syntactic information and only using local inference. However, when automatically predicted part-of-speech tags are provided as input, it substantially outperforms all previous local models and approaches the best reported results on the CoNLL-2009 dataset. Syntactic parsers are unreliable on out-of-domain data, so standard (i.e. syntactically-informed) SRL models are hindered when tested in this setting. Our syntax-agnostic model appears more robust, resulting in the best reported results on the standard out-of-domain test set.

**URL:** https://arxiv.org/abs/1701.02593

**Notes:** New paper from Russian guys about PoS-tagging, could be useful in dialog tracking maybe.

### Reinforcement Learning via Recurrent Convolutional Neural Networks

**Authors:** Tanmay Shankar, Santosha K. Dwivedy, Prithwijit Guha

**Abstract:** Deep Reinforcement Learning has enabled the learning of policies for complex tasks in partially observable environments, without explicitly learning the underlying model of the tasks. While such model-free methods achieve considerable performance, they often ignore the structure of task. We present a natural representation of to Reinforcement Learning (RL) problems using Recurrent Convolutional Neural Networks (RCNNs), to better exploit this inherent structure. We define 3 such RCNNs, whose forward passes execute an efficient Value Iteration, propagate beliefs of state in partially observable environments, and choose optimal actions respectively. Backpropagating gradients through these RCNNs allows the system to explicitly learn the Transition Model and Reward Function associated with the underlying MDP, serving as an elegant alternative to classical model-based RL. We evaluate the proposed algorithms in simulation, considering a robot planning problem. We demonstrate the capability of our framework to reduce the cost of replanning, learn accurate MDP models, and finally re-plan with learnt models to achieve near-optimal policies.

**URL:** https://arxiv.org/abs/1701.02392

**Notes:** Reccurent CNN is a new trend, they had shown themselves as useful tool in NLP, now in RL, couldn't miss this one.

### Visualizing Residual Networks

**Authors:** Brian Chu, Daylen Yang, Ravi Tadinada

**Abstract:** Residual networks are the current state of the art on ImageNet. Similar work in the direction of utilizing shortcut connections has been done extremely recently with derivatives of residual networks and with highway networks. This work potentially challenges our understanding that CNNs learn layers of local features that are followed by increasingly global features. Through qualitative visualization and empirical analysis, we explore the purpose that residual skip connections serve. Our assessments show that the residual shortcut connections force layers to refine features, as expected. We also provide alternate visualizations that confirm that residual networks learn what is already intuitively known about CNNs in general.

**URL:** https://arxiv.org/abs/1701.02362

**Notes:** The heading is talking for itself, could be useful due to residuality now is useful everywhere: RNN, CNN, etc.

### Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities

**Authors:** Yadollah Yaghoobzadeh, Hinrich Schütze

**Abstract:** Entities are essential elements of natural language. In this paper, we present methods for learning multi-level representations of entities on three complementary levels: character (character patterns in entity names extracted, e.g., by neural networks), word (embeddings of words in entity names) and entity (entity embeddings). We investigate state-of-the-art learning methods on each level and find large differences, e.g., for deep learning models, traditional ngram features and the subword model of fasttext (Bojanowski et al., 2016) on the character level; for word2vec (Mikolov et al., 2013) on the word level; and for the order-aware model wang2vec (Ling et al., 2015a) on the entity level. We confirm experimentally that each level of representation contributes complementary information and a joint representation of all three levels improves the existing embedding based baseline for fine-grained entity typing by a large margin. Additionally, we show that adding information from entity descriptions further improves multi-level representations of entities.

**URL:** https://arxiv.org/abs/1701.02025

**Notes:** fresh paper from Schuetze, triune of Char, Word, & Entity, seems to be the part of NLP Holy Grail

### Neural Personalized Response Generation as Domain Adaptation

**Authors:** Weinan Zhang, Ting Liu, Yifa Wang, Qingfu Zhu

**Abstract:** In this paper, we focus on the personalized response generation for conversational systems. Based on the sequence to sequence learning, especially the encoder-decoder framework, we propose a two-phase approach, namely initialization then adaptation, to model the responding style of human and then generate personalized responses. For evaluation, we propose a novel human aided method to evaluate the performance of the personalized response generation models by online real-time conversation and offline human judgement. Moreover, the lexical divergence of the responses generated by the 5 personalized models indicates that the proposed two-phase approach achieves good results on modeling the responding style of human and generating personalized responses for the conversational systems.

**URL:** https://arxiv.org/abs/1701.02073

**Notes:** personalized answer is really important part for seamless conversation, training the style from responses is a nice idea.

### Task-Specific Attentive Pooling of Phrase Alignments Contributes to Sentence Matching

**Authors:** Wenpeng Yin, Hinrich Schütze

**Abstract:** This work studies comparatively two typical sentence matching tasks: textual entailment (TE) and answer selection (AS), observing that weaker phrase alignments are more critical in TE, while stronger phrase alignments deserve more attention in AS. The key to reach this observation lies in phrase detection, phrase representation, phrase alignment, and more importantly how to connect those aligned phrases of different matching degrees with the final classifier. Prior work (i) has limitations in phrase generation and representation, or (ii) conducts alignment at word and phrase levels by handcrafted features or (iii) utilizes a single framework of alignment without considering the characteristics of specific tasks, which limits the framework's effectiveness across tasks. We propose an architecture based on Gated Recurrent Unit that supports (i) representation learning of phrases of arbitrary granularity and (ii) task-specific attentive pooling of phrase alignments between two sentences. Experimental results on TE and AS match our observation and show the effectiveness of our approach.

**URL:** https://arxiv.org/abs/1701.02149

**Notes:** attentive pooling for NLP tasks is very hot

### Structural Attention Neural Networks for improved sentiment analysis

**Authors:** Filippos Kokkinos, Alexandros Potamianos

**Abstract:** We introduce a tree-structured attention neural network for sentences and small phrases and apply it to the problem of sentiment classification. Our model expands the current recursive models by incorporating structural information around a node of a syntactic tree using both bottom-up and top-down information propagation. Also, the model utilizes structural attention to identify the most salient representations during the construction of the syntactic tree. To our knowledge, the proposed models achieve state of the art performance on the Stanford Sentiment Treebank dataset.

**URL:** https://arxiv.org/abs/1701.01811

**Notes:** one more attention type

### Self-Taught Convolutional Neural Networks for Short Text Clustering

**Authors:** Jiaming Xu, Bo Xu, Peng Wang, Suncong Zheng, Guanhua Tian, Jun Zhao, Bo Xu

**Abstract:** Short text clustering is a challenging problem due to its sparseness of text representation. Here we propose a flexible Self-Taught Convolutional neural network framework for Short Text Clustering (dubbed STC^2), which can flexibly and successfully incorporate more useful semantic features and learn non-biased deep text representation in an unsupervised manner. In our framework, the original raw text features are firstly embedded into compact binary codes by using one existing unsupervised dimensionality reduction methods. Then, word embeddings are explored and fed into convolutional neural networks to learn deep feature representations, meanwhile the output units are used to fit the pre-trained binary codes in the training process. Finally, we get the optimal clusters by employing K-means to cluster the learned representations. Extensive experimental results demonstrate that the proposed framework is effective, flexible and outperform several popular clustering methods when tested on three public short text datasets.

**URL:** https://arxiv.org/abs/1701.00185

**Notes:** unsupervised clusteding by CNNs!

### Textual Entailment with Structured Attentions and Composition

**Authors:** Kai Zhao, Liang Huang, Mingbo Ma

**Abstract:** Deep learning techniques are increasingly popular in the textual entailment task, overcoming the fragility of traditional discrete models with hard alignments and logics. In particular, the recently proposed attention models (Rockt\"aschel et al., 2015; Wang and Jiang, 2015) achieves state-of-the-art accuracy by computing soft word alignments between the premise and hypothesis sentences. However, there remains a major limitation: this line of work completely ignores syntax and recursion, which is helpful in many traditional efforts. We show that it is beneficial to extend the attention model to tree nodes between premise and hypothesis. More importantly, this subtree-level attention reveals information about entailment relation. We study the recursive composition of this subtree-level entailment relation, which can be viewed as a soft version of the Natural Logic framework (MacCartney and Manning, 2009). Experiments show that our structured attention and entailment composition model can correctly identify and infer entailment relations from the bottom up, and bring significant improvements in accuracy.

**URL:** https://arxiv.org/abs/1701.01126

**Notes:** yet another attention

### Unsupervised neural and Bayesian models for zero-resource speech processing

**Authors:** Herman Kamper

**Abstract:** In settings where only unlabelled speech data is available, zero-resource speech technology needs to be developed without transcriptions, pronunciation dictionaries, or language modelling text. There are two central problems in zero-resource speech processing: (i) finding frame-level feature representations which make it easier to discriminate between linguistic units (phones or words), and (ii) segmenting and clustering unlabelled speech into meaningful units. In this thesis, we argue that a combination of top-down and bottom-up modelling is advantageous in tackling these two problems. To address the problem of frame-level representation learning, we present the correspondence autoencoder (cAE), a neural network trained with weak top-down supervision from an unsupervised term discovery system. By combining this top-down supervision with unsupervised bottom-up initialization, the cAE yields much more discriminative features than previous approaches. We then present our unsupervised segmental Bayesian model that segments and clusters unlabelled speech into hypothesized words. By imposing a consistent top-down segmentation while also using bottom-up knowledge from detected syllable boundaries, our system outperforms several others on multi-speaker conversational English and Xitsonga speech data. Finally, we show that the clusters discovered by the segmental Bayesian model can be made less speaker- and gender-specific by using features from the cAE instead of traditional acoustic features. In summary, the different models and systems presented in this thesis show that both top-down and bottom-up modelling can improve representation learning, segmentation and clustering of unlabelled speech data.

**URL:** https://arxiv.org/abs/1701.00851

**Notes:** Unsurervised neural vs Bayesian approahces in speech processing

### NIPS 2016 Tutorial: Generative Adversarial Networks

**Authors:** Ian Goodfellow

**Abstract:** This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.

**URL:** https://arxiv.org/abs/1701.00160

**Notes:** Goodfellow's tutorial couldn't hurt

### Dense Associative Memory is Robust to Adversarial Inputs

**Authors:** Dmitry Krotov, John J Hopfield

**Abstract:** Deep neural networks (DNN) trained in a supervised way suffer from two known problems. First, the minima of the objective function used in learning correspond to data points (also known as rubbish examples or fooling images) that lack semantic similarity with the training data. Second, a clean input can be changed by a small, and often imperceptible for human vision, perturbation, so that the resulting deformed input is misclassified by the network. These findings emphasize the differences between the ways DNN and humans classify patterns, and raise a question of designing learning algorithms that more accurately mimic human perception compared to the existing methods. Our paper examines these questions within the framework of Dense Associative Memory (DAM) models. These models are defined by the energy function, with higher order (higher than quadratic) interactions between the neurons. We show that in the limit when the power of the interaction vertex in the energy function is sufficiently large, these models have the following three properties. First, the minima of the objective function are free from rubbish images, so that each minimum is a semantically meaningful pattern. Second, artificial patterns poised precisely at the decision boundary look ambiguous to human subjects and share aspects of both classes that are separated by that decision boundary. Third, adversarial images constructed by models with small power of the interaction vertex, which are equivalent to DNN with rectified linear units (ReLU), fail to transfer to and fool the models with higher order interactions. This opens up a possibility to use higher order models for detecting and stopping malicious adversarial attacks. The presented results suggest that DAM with higher order energy functions are closer to human visual perception than DNN with ReLUs.

**URL:** https://arxiv.org/abs/1701.00939

**Notes:** The memory paper from *that* Hopfield

### A K-fold Method for Baseline Estimation in Policy Gradient Algorithms

**Authors:** Nithyanand Kota, Abhishek Mishra, Sunil Srinivasa, Xi (Peter) Chen, Pieter Abbeel

**Abstract:** The high variance issue in unbiased policy-gradient methods such as VPG and REINFORCE is typically mitigated by adding a baseline. However, the baseline fitting itself suffers from the underfitting or the overfitting problem. In this paper, we develop a K-fold method for baseline estimation in policy gradient algorithms. The parameter K is the baseline estimation hyperparameter that can adjust the bias-variance trade-off in the baseline estimates. We demonstrate the usefulness of our approach via two state-of-the-art policy gradient algorithms on three MuJoCo locomotive control tasks.

**URL:** https://arxiv.org/abs/1701.00867

**Notes:** Simple baseline for policy gradient

### Generating Long and Diverse Responses with Neural Conversation Models

**Authors:** Louis Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, Ray Kurzweil

**Abstract:** Building general-purpose conversation agents is a very challenging task, but necessary on the road toward intelligent agents that can interact with humans in natural language. Neural conversation models — purely data-driven systems trained end-to-end on dialogue corpora — have shown great promise recently, yet they often produce short and generic responses. This work presents new training and decoding methods that improve the quality, coherence, and diversity of long responses generated using sequence-to-sequence models. Our approach adds self-attention to the decoder to maintain coherence in longer responses, and we propose a practical approach, called the glimpse-model, for scaling to large datasets. We introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process. We trained on a combined data set of over 2.3B conversation messages mined from the web. In human evaluation studies, our method produces longer responses overall, with a higher proportion rated as acceptable and excellent as length increases, compared to baseline sequence-to-sequence models with explicit length-promotion. A back-off strategy produces better responses overall, in the full spectrum of lengths.

**URL:** https://arxiv.org/abs/1701.03185

**Notes:** more diversity for responces, we should look over this work, since the beam search isn't satisfying

### Simplified Gating in Long Short-term Memory (LSTM) Recurrent Neural Networks

**Authors:** Yuzhen Lu, Fathi M. Salem

**Abstract:** The standard LSTM recurrent neural networks while very powerful in long-range dependency sequence applications have highly complex structure and relatively large (adaptive) parameters. In this work, we present empirical comparison between the standard LSTM recurrent neural network architecture and three new parameter-reduced variants obtained by eliminating combinations of the input signal, bias, and hidden unit signals from individual gating signals. The experiments on two sequence datasets show that the three new variants, called simply as LSTM1, LSTM2, and LSTM3, can achieve comparable performance to the standard LSTM model with less (adaptive) parameters.

**URL:** https://arxiv.org/abs/1701.03441

**Notes:** that's a pity, I had the similar idea, you need to go fast with trying ideas this days!

### Modularized Morphing of Neural Networks

**Authors:** Tao Wei, Changhu Wang, Chang Wen Chen

**Abstract:** In this work we study the problem of network morphism, an effective learning scheme to morph a well-trained neural network to a new one with the network function completely preserved. Different from existing work where basic morphing types on the layer level were addressed, we target at the central problem of network morphism at a higher level, i.e., how a convolutional layer can be morphed into an arbitrary module of a neural network. To simplify the representation of a network, we abstract a module as a graph with blobs as vertices and convolutional layers as edges, based on which the morphing process is able to be formulated as a graph transformation problem. Two atomic morphing operations are introduced to compose the graphs, based on which modules are classified into two families, i.e., simple morphable modules and complex modules. We present practical morphing solutions for both of these two families, and prove that any reasonable module can be morphed from a single convolutional layer. Extensive experiments have been conducted based on the state-of-the-art ResNet on benchmark datasets, and the effectiveness of the proposed solution has been verified.

**URL:** https://arxiv.org/abs/1701.03281

**Notes:** modularization is a fresh idea, I cannot get morphing aside the brain damage (zeroing small weights) yet

### A Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on Task-Oriented Dialogue

**Authors:** Mihail Eric, Christopher D. Manning

**Abstract:** Task-oriented dialogue focuses on conversational agents that participate in user-initiated dialogues on domain-specific topics. In contrast to chatbots, which simply seek to sustain open-ended meaningful discourse, existing task-oriented agents usually explicitly model user intent and belief states. This paper examines bypassing such an explicit representation by depending on a latent neural embedding of state and learning selective attention to dialogue history together with copying to incorporate relevant prior context. We complement recent work by showing the effectiveness of simple sequence-to-sequence neural architectures with a copy mechanism. Our model outperforms more complex memory-augmented models by 7% in per-response generation and is on par with the current state-of-the-art on DSTC2.

**URL:** https://arxiv.org/abs/1701.04024

**Notes:** Seq2Seq still has some tricks up its sleeve, copying as context is a bright idea

### Dialog Context Language Modeling with Recurrent Neural Networks

**Authors:** Bing Liu, Ian Lane

**Abstract:** In this work, we propose contextual language models that incorporate dialog level discourse information into language modeling. Previous works on contextual language model treat preceding utterances as a sequence of inputs, without considering dialog interactions. We design recurrent neural network (RNN) based contextual language models that specially track the interactions between speakers in a dialog. Experiment results on Switchboard Dialog Act Corpus show that the proposed model outperforms conventional single turn based RNN language model by 3.3% on perplexity. The proposed models also demonstrate advantageous performance over other competitive contextual language models.

**URL:** https://arxiv.org/abs/1701.04056

**Notes:** Another context incorporation with RNN

### Neural Models for Sequence Chunking

**Authors:** Feifei Zhai, Saloni Potdar, Bing Xiang, Bowen Zhou

**Abstract:** Many natural language understanding (NLU) tasks, such as shallow parsing (i.e., text chunking) and semantic slot filling, require the assignment of representative labels to the meaningful chunks in a sentence. Most of the current deep neural network (DNN) based methods consider these tasks as a sequence labeling problem, in which a word, rather than a chunk, is treated as the basic unit for labeling. These chunks are then inferred by the standard IOB (Inside-Outside-Beginning) labels. In this paper, we propose an alternative approach by investigating the use of DNN for sequence chunking, and propose three neural models so that each chunk can be treated as a complete unit for labeling. Experimental results show that the proposed neural sequence chunking models can achieve start-of-the-art performance on both the text chunking and slot filling tasks.

**URL:** https://arxiv.org/abs/1701.04027

**Notes:** Sequence chunking is common task for asian languages in the first place, but since we are going to go chars, for european ones too

### DyNet: The Dynamic Neural Network Toolkit

**Authors:** Graham Neubig, Chris Dyer, Yoav Goldberg, Austin Matthews, Waleed Ammar, Antonios Anastasopoulos, Miguel Ballesteros, David Chiang, Daniel Clothiaux, Trevor Cohn, Kevin Duh, Manaal Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji, Lingpeng Kong, Adhiguna Kuncoro, Gaurav Kumar, Chaitanya Malaviya, Paul Michel, Yusuke Oda, Matthew Richardson, Naomi Saphra, Swabha Swayamdipta, Pengcheng Yin

**Abstract:** We describe DyNet, a toolkit for implementing neural network models based on dynamic declaration of network structure. In the static declaration strategy that is used in toolkits like Theano, CNTK, and TensorFlow, the user first defines a computation graph (a symbolic representation of the computation), and then examples are fed into an engine that executes this computation and computes its derivatives. In DyNet's dynamic declaration strategy, computation graph construction is mostly transparent, being implicitly constructed by executing procedural code that computes the network outputs, and the user is free to use different network structures for each input. Dynamic declaration thus facilitates the implementation of more complicated network architectures, and DyNet is specifically designed to allow users to implement their models in a way that is idiomatic in their preferred programming language (C++ or Python). One challenge with dynamic declaration is that because the symbolic computation graph is defined anew for every training example, its construction must have low overhead. To achieve this, DyNet has an optimized C++ backend and lightweight graph representation. Experiments show that DyNet's speeds are faster than or comparable with static declaration toolkits, and significantly faster than Chainer, another dynamic declaration toolkit. DyNet is released open-source under the Apache 2.0 license and available at this http URL

**URL:** https://arxiv.org/abs/1701.03980

**Notes:** The paper has remarkable list of authors - DeepMind, Google, IBM Watson, CMU, AI2 & MSR. And more! Very interesting initiative.

### Understanding the Effective Receptive Field in Deep Convolutional Neural Networks

**Authors:** Wenjie Luo, Yujia Li, Raquel Urtasun, Richard Zemel

**Abstract:** We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small.

**URL:** https://arxiv.org/abs/1701.04128

**Notes:** The topic I was always curious about, shoud read it carefully, since now CNN are in rising at NLP field

### Agent-Agnostic Human-in-the-Loop Reinforcement Learning

**Authors:** David Abel, John Salvatier, Andreas Stuhlmüller, Owain Evans

**Abstract:** Providing Reinforcement Learning agents with expert advice can dramatically improve various aspects of learning. Prior work has developed teaching protocols that enable agents to learn efficiently in complex environments; many of these methods tailor the teacher's guidance to agents with a particular representation or underlying learning scheme, offering effective but specialized teaching procedures. In this work, we explore protocol programs, an agent-agnostic schema for Human-in-the-Loop Reinforcement Learning. Our goal is to incorporate the beneficial properties of a human teacher into Reinforcement Learning without making strong assumptions about the inner workings of the agent. We show how to represent existing approaches such as action pruning, reward shaping, and training in simulation as special cases of our schema and conduct preliminary experiments on simple domains.

**URL:** https://arxiv.org/abs/1701.04079

**Notes:** Next step for FAIR human in the loop approach

### Minimally Naturalistic Artificial Intelligence

**Authors:** Steven Stenberg Hansen

**Abstract:** The rapid advancement of machine learning techniques has re-energized research into general artificial intelligence. While the idea of domain-agnostic meta-learning is appealing, this emerging field must come to terms with its relationship to human cognition and the statistics and structure of the tasks humans perform. The position of this article is that only by aligning our agents' abilities and environments with those of humans do we stand a chance at developing general artificial intelligence (GAI). A broad reading of the famous 'No Free Lunch' theorem is that there is no universally optimal inductive bias or, equivalently, bias-free learning is impossible. This follows from the fact that there are an infinite number of ways to extrapolate data, any of which might be the one used by the data generating environment; an inductive bias prefers some of these extrapolations to others, which lowers performance in environments using these adversarial extrapolations. We may posit that the optimal GAI is the one that maximally exploits the statistics of its environment to create its inductive bias; accepting the fact that this agent is guaranteed to be extremely sub-optimal for some alternative environments. This trade-off appears benign when thinking about the environment as being the physical universe, as performance on any fictive universe is obviously irrelevant. But, we should expect a sharper inductive bias if we further constrain our environment. Indeed, we implicitly do so by defining GAI in terms of accomplishing that humans consider useful. One common version of this is need the for 'common-sense reasoning', which implicitly appeals to the statistics of physical universe as perceived by humans.

**URL:** https://arxiv.org/abs/1701.03868

**Notes:** Seems to be little bit too loud name, but we should check inside.

### Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks

**Authors:** Lars Mescheder, Sebastian Nowozin, Andreas Geiger

**Abstract:** Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model used during training. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.

**URL:** https://arxiv.org/abs/1701.04722

**Notes:** Convergence of autoencoders & GANs, neat!

### Adversarial Learning for Neural Dialogue Generation

**Authors:** Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, Dan Jurafsky

**Abstract:** In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator---analagous to the human evaluator in the Turing test-— to distinguish between the human-generated dialogues and the machine-generated ones. The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues. In addition to adversarial training we describe a model for adversarial {\em evaluation} that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines.

**URL:** https://arxiv.org/abs/1701.06547

**Notes:** HOT! GAN-RL!

### A Multichannel Convolutional Neural Network For Cross-language Dialog State Tracking

**Authors:** Hongjie Shi, Takashi Ushio, Mitsuru Endo, Katsuyoshi Yamagami, Noriaki Horii

**Abstract:** The fifth Dialog State Tracking Challenge (DSTC5) introduces a new cross-language dialog state tracking scenario, where the participants are asked to build their trackers based on the English training corpus, while evaluating them with the unlabeled Chinese corpus. Although the computer-generated translations for both English and Chinese corpus are provided in the dataset, these translations contain errors and careless use of them can easily hurt the performance of the built trackers. To address this problem, we propose a multichannel Convolutional Neural Networks (CNN) architecture, in which we treat English and Chinese language as different input channels of one single CNN model. In the evaluation of DSTC5, we found that such multichannel architecture can effectively improve the robustness against translation errors. Additionally, our method for DSTC5 is purely machine learning based and requires no prior knowledge about the target language. We consider this a desirable property for building a tracker in the cross-language context, as not every developer will be familiar with both languages.

**URL:** https://arxiv.org/abs/1701.06247

**Notes:** CNN for dialog state tracking

### Learning to Decode for Future Success

**Authors:** Jiwei Li, Will Monroe, Dan Jurafsky

**Abstract:** We introduce a general strategy for improving neural sequence generation by incorporating knowledge about the future. Our decoder combines a standard sequence decoder with a `soothsayer' prediction function Q that estimates the outcome in the future of generating a word in the present. Our model draws on the same intuitions as reinforcement learning, but is both simpler and higher performing, avoiding known problems with the use of reinforcement learning in tasks with enormous search spaces like sequence generation. We demonstrate our model by incorporating Q functions that incrementally predict what the future BLEU or ROUGE score of the completed sequence will be, its future length, and the backwards probability of the source given the future target sequence. Experimental results show that future rediction yields improved performance in abstractive summarization and conversational response generation and the state-of-the-art in machine translation, while also enabling the decoder to generate outputs that have specific properties.

**URL:** https://arxiv.org/abs/1701.06549

**Notes:** future prediction with Q-learning

### Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer

**Authors:** Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean

**Abstract:** The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.

**URL:** https://arxiv.org/abs/1701.06538

**Notes:** dropout analog from Dean & Hinton

### Regularizing Neural Networks by Penalizing Confident Output Distributions

**Authors:** Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, Geoffrey Hinton

**Abstract:** We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. Furthermore, we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and the confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyperparameters, suggesting the wide applicability of these regularizers.

**URL:** https://arxiv.org/abs/1701.06548

**Notes:** smart regularization from Hinton

### Discriminative Neural Topic Models

**Authors:** Gaurav Pandey, Ambedkar Dukkipati

**Abstract:** We propose a neural network based approach for learning topics from text and image datasets. The model makes no assumptions about the conditional distribution of the observed features given the latent topics. This allows us to perform topic modelling efficiently using sentences of documents and patches of images as observed features, rather than limiting ourselves to words. Moreover, the proposed approach is online, and hence can be used for streaming data. Furthermore, since the approach utilizes neural networks, it can be implemented on GPU with ease, and hence it is very scalable.

**URL:** https://arxiv.org/abs/1701.06796

**Notes:** don't like topic modeling but you should stay in touch with advances these days

### Hierarchical Recurrent Attention Network for Response Generation

**Authors:** Chen Xing, Wei Wu, Yu Wu, Ming Zhou, Yalou Huang, Wei-Ying Ma

**Abstract:** We study multi-turn response generation in chatbots where a response is generated according to a conversation context. Existing work has modeled the hierarchy of the context, but does not pay enough attention to the fact that words and utterances in the context are differentially important. As a result, they may lose important information in context and generate irrelevant responses. We propose a hierarchical recurrent attention network (HRAN) to model both aspects in a unified framework. In HRAN, a hierarchical attention mechanism attends to important parts within and among utterances with word level attention and utterance level attention respectively. With the word level attention, hidden vectors of a word level encoder are synthesized as utterance vectors and fed to an utterance level encoder to construct hidden representations of the context. The hidden vectors of the context are then processed by the utterance level attention and formed as context vectors for decoding the response. Empirical studies on both automatic evaluation and human judgment show that HRAN can significantly outperform state-of-the-art models for multi-turn response generation.

**URL:** https://arxiv.org/abs/1701.07149

**Notes:** responce genration with hierarchical network is not that fresh idea, but may be these guys have the results

### Wasserstein GAN

**Authors:** Martin Arjovsky, Soumith Chintala, Léon Bottou

**Abstract:** We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.

**URL:** https://arxiv.org/abs/1701.07875

**Notes:** New enhancement for GANs, could be useful.

### Reinforced backpropagation improves test performance of deep networks: a toy-model study

**Authors:** Haiping Huang, Taro Toyoizumi

**Abstract:** Standard error backpropagation is used in almost all modern deep network training. However, it typically suffers from proliferation of saddle points in high-dimensional parameter space. Therefore, it is highly desirable to design an efficient algorithm to escape from these saddle points and reach a good parameter region of better generalization capabilities, especially based on rough insights about the landscape of the error surface. Here, we propose a simple extension of the backpropagation, namely reinforced backpropagation, which simply adds previous first-order gradients in a stochastic manner with a probability that increases with learning time. Extensive numerical simulations on a toy deep learning model verify its excellent performance. The reinforced backpropagation can significantly improve test performance of the deep network training, especially when the data are scarce. The performance is even better than that of state-of-the-art stochastic optimization algorithm called Adam, with an extra advantage of less computer memory required.

**URL:** https://arxiv.org/abs/1701.07974

**Notes:** seems to be analogue for momentum in SGD, could be helpful may be

### CommAI: Evaluating the first steps towards a useful general AI

**Authors:** Marco Baroni, Armand Joulin, Allan Jabri, Germàn Kruszewski, Angeliki Lazaridou, Klemen Simonic, Tomas Mikolov

**Abstract:** With machine learning successfully applied to new daunting problems almost every day, general AI starts looking like an attainable goal. However, most current research focuses instead on important but narrow applications, such as image classification or machine translation. We believe this to be largely due to the lack of objective ways to measure progress towards broad machine intelligence. In order to fill this gap, we propose here a set of concrete desiderata for general AI, together with a platform to test machines on how well they satisfy such desiderata, while keeping all further complexities to a minimum.

**URL:** https://arxiv.org/abs/1701.08954

**Notes:** Fresh article from Mikolov about actual path to general AI.

## 2017-02
### Low-Dose CT with a Residual Encoder-Decoder Convolutional Neural Network (RED-CNN)

**Authors:** Hu Chen, Yi Zhang, Mannudeep K. Kalra, Feng Lin, Peixi Liao, Jiliu Zhou, Ge Wang

**Abstract:** Given the potential X-ray radiation risk to the patient, low-dose CT has attracted a considerable interest in the medical imaging field. The current main stream low-dose CT methods include vendor-specific sinogram domain filtration and iterative reconstruction, but they need to access original raw data whose formats are not transparent to most users. Due to the difficulty of modeling the statistical characteristics in the image domain, the existing methods for directly processing reconstructed images cannot eliminate image noise very well while keeping structural details. Inspired by the idea of deep learning, here we combine the autoencoder, the deconvolution network, and shortcut connections into the residual encoder-decoder convolutional neural network (RED-CNN) for low-dose CT imaging. After patch-based training, the proposed RED-CNN achieves a competitive performance relative to the-state-of-art methods in both simulated and clinical cases. Especially, our method has been favorably evaluated in terms of noise suppression, structural preservation and lesion detection.

**URL:** https://arxiv.org/abs/1702.00288

**Notes:** similar architecture we've used for sentence representation task

### On orthogonality and learning recurrent networks with long term dependencies

**Authors:** Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, Chris Pal

**Abstract:** It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and can therefore be a desirable property; however, we find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance. This paper explores the issues of optimization convergence, speed and gradient stability using a variety of different methods for encouraging or enforcing orthogonality. In particular we propose a weight matrix factorization and parameterization strategy through which we can bound matrix norms and therein control the degree of expansivity induced during backpropagation.

**URL:** https://arxiv.org/abs/1702.00071

**Notes:** long-term dependencies for text are really important, so I should this check out

### Understanding trained CNNs by indexing neuron selectivity

**Authors:** Ivet Rafegas, Maria Vanrell, Luis A. Alexandre

**Abstract:** The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifying their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties. Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful.

**URL:** https://arxiv.org/abs/1702.00382

**Notes:** 1st article about understanding CNN today

### Design, Analysis and Application of A Volumetric Convolutional Neural Network

**Authors:** Xiaqing Pan, Yueru Chen, C.-C. Jay Kuo

**Abstract:** The design, analysis and application of a volumetric convolutional neural network (VCNN) are studied in this work. Although many CNNs have been proposed in the literature, their design is empirical. In the design of the VCNN, we propose a feed-forward K-means clustering algorithm to determine the filter number and size at each convolutional layer systematically. For the analysis of the VCNN, the cause of confusing classes in the output of the VCNN is explained by analyzing the relationship between the filter weights (also known as anchor vectors) from the last fully-connected layer to the output. Furthermore, a hierarchical clustering method followed by a random forest classification method is proposed to boost the classification performance among confusing classes. For the application of the VCNN, we examine the 3D shape classification problem and conduct experiments on a popular ModelNet40 dataset. The proposed VCNN offers the state-of-the-art performance among all volume-based CNN methods.

**URL:** https://arxiv.org/abs/1702.00158

**Notes:** 2nd article about understanding CNN

### On SGD's Failure in Practice: Characterizing and Overcoming Stalling

**Authors:** Vivak Patel

**Abstract:** Stochastic Gradient Descent (SGD) is widely used in machine learning problems to efficiently perform empirical risk minimization, yet, in practice, SGD is known to stall before reaching the actual minimizer of the empirical risk. SGD stalling has often been attributed to its sensitivity to the conditioning of the problem; however, as we demonstrate, SGD will stall even when applied to a simple linear regression problem with unity condition number for standard learning rates. Thus, in this work, we numerically demonstrate and mathematically argue that stalling is a crippling and generic limitation of SGD and its variants in practice. Once we have established the problem of stalling, we introduce a framework for hedging against its effects, which (1) deters SGD and its variants from stalling, (2) still provides convergence guarantees, and (3) makes SGD and its variants more practical methods for minimization.

**URL:** https://arxiv.org/abs/1702.00317

**Notes:** SGD failures should be understood to work with it

### Symbolic, Distributed and Distributional Representations for Natural Language Processing in the Era of Deep Learning: a Survey

**Authors:** Lorenzo Ferrone, Fabio Massimo Zanzotto

**Abstract:** Natural language and symbols are intimately correlated. Recent advances in machine learning (ML) and in natural language processing (NLP) seem to contradict the above intuition: symbols are fading away, erased by vectors or tensors called distributed and distributional representations. However, there is a strict link between distributed/distributional representations and symbols, being the first an approximation of the second. A clearer understanding of the strict link between distributed/distributional representations and symbols will certainly lead to radically new deep learning networks. In this paper we make a survey that aims to draw the link between symbolic representations and distributed/distributional representations. This is the right time to revitalize the area of interpreting how symbols are represented inside neural networks.

**URL:** https://arxiv.org/abs/1702.00764

**Notes:** review of nlp representations

### Multilingual Multi-modal Embeddings for Natural Language Processing

**Authors:** Iacer Calixto, Qun Liu, Nick Campbell

**Abstract:** We propose a novel discriminative model that learns embeddings from multilingual and multi-modal data, meaning that our model can take advantage of images and descriptions in multiple languages to improve embedding quality. To that end, we introduce a modification of a pairwise contrastive estimation optimisation function as our training objective. We evaluate our embeddings on an image-sentence ranking (ISR), a semantic textual similarity (STS), and a neural machine translation (NMT) task. We find that the additional multilingual signals lead to improvements on both the ISR and STS tasks, and the discriminative cost can also be used in re-ranking n-best lists produced by NMT models, yielding strong improvements.

**URL:** https://arxiv.org/abs/1702.01101

**Notes:** yet another embedding, should hurry to publish mine

### Doubly-Attentive Decoder for Multi-modal Neural Machine Translation

**Authors:** Iacer Calixto, Qun Liu, Nick Campbell

**Abstract:** We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.

**URL:** https://arxiv.org/abs/1702.01287

**Notes:** double attention for NMT

### Opinion Recommendation using Neural Memory Model

**Authors:** Zhongqing Wang, Yue Zhang

**Abstract:** We present opinion recommendation, a novel task of jointly predicting a custom review with a rating score that a certain user would give to a certain product or service, given existing reviews and rating scores to the product or service by other users, and the reviews that the user has given to other products and services. A characteristic of opinion recommendation is the reliance of multiple data sources for multi-task joint learning, which is the strength of neural models. We use a single neural network to model users and products, capturing their correlation and generating customised product representations using a deep memory network, from which customised ratings and reviews are constructed jointly. Results show that our opinion recommendation system gives ratings that are closer to real user ratings on Yelp.com data compared with Yelp's own ratings, and our methods give better results compared to several pipelines baselines using state-of-the-art sentiment rating and summarization systems.

**URL:** https://arxiv.org/abs/1702.01517

**Notes:** opinion recommendation - neмук seen such task before

### Search Intelligence: Deep Learning For Dominant Category Prediction

**Authors:** Zeeshan Khawar Malik, Mo Kobrosli, Peter Maas

**Abstract:** Deep Neural Networks, and specifically fully-connected convolutional neural networks are achieving remarkable results across a wide variety of domains. They have been trained to achieve state-of-the-art performance when applied to problems such as speech recognition, image classification, natural language processing and bioinformatics. Most of these deep learning models when applied to classification employ the softmax activation function for prediction and aim to minimize cross-entropy loss. In this paper, we have proposed a supervised model for dominant category prediction to improve search recall across all eBay classifieds platforms. The dominant category label for each query in the last 90 days is first calculated by summing the total number of collaborative clicks among all categories. The category having the highest number of collaborative clicks for the given query will be considered its dominant category. Second, each query is transformed to a numeric vector by mapping each unique word in the query document to a unique integer value; all padded to equal length based on the maximum document length within the pre-defined vocabulary size. A fully-connected deep convolutional neural network (CNN) is then applied for classification. The proposed model achieves very high classification accuracy compared to other state-of-the-art machine learning techniques.

**URL:** https://arxiv.org/abs/1702.01717

**Notes:** CNN for classification is classic approach now, but this thing is working with click streams

### Syntax-aware Neural Machine Translation Using CCG

**Authors:** Maria Nadejde, Siva Reddy, Rico Sennrich, Tomasz Dwojak, Marcin Junczys-Dowmunt, Philipp Koehn, Alexandra Birch

**Abstract:** Neural machine translation (NMT) models are able to partially learn syntactic information from sequential lexical information. Still, some complex syntactic phenomena such as prepositional phrase attachment are poorly modeled. This work aims to answer two questions: 1) Does explicitly modeling source or target language syntax help NMT? 2) Is tight integration of words and syntax better than multitask training? We introduce syntactic information in the form of CCG supertags either in the source as an extra feature in the embedding, or in the target, by interleaving the target supertags with the word sequence. Our results on WMT data show that explicitly modeling syntax improves machine translation quality for English-German, a high-resource pair, and for English-Romanian, a low-resource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training.

**URL:** https://arxiv.org/abs/1702.01147

**Notes:** NMT with syntax awareness from Philipp Koehn

### Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks

**Authors:** Guy Katz, Clark Barrett, David Dill, Kyle Julian, Mykel Kochenderfer

**Abstract:** Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation Airborne Collision Avoidance System for unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods.

**URL:** https://arxiv.org/abs/1702.01135

**Notes:** SMT based on collision avoidance system that is something new

### Neural Semantic Parsing over Multiple Knowledge-bases

**Authors:** Jonathan Herzig, Jonathan Berant

**Abstract:** A fundamental challenge in developing semantic parsers is the paucity of strong supervision in the form of language utterances annotated with logical form. In this paper, we propose to exploit structural regularities in language in different domains, and train semantic parsers over multiple knowledge-bases (KBs), while sharing information across datasets. We find that we can substantially improve parsing accuracy by training a single sequence-to-sequence model over multiple KBs, when providing an encoding of the domain at decoding time. Our model achieves state-of-the-art performance on the Overnight dataset (containing eight domains), improves performance over a single KB baseline from 75.6% to 79.6%, while obtaining a 7x reduction in the number of model parameters.

**URL:** https://arxiv.org/abs/1702.01569

**Notes:** KBs are generally not my area, but this worth to check out since they propose simplification

### All-but-the-Top: Simple and Effective Postprocessing for Word Representations

**Authors:** Jiaqi Mu, Suma Bhat, Pramod Viswanath

**Abstract:** Real-valued word representations have transformed NLP applications, popular examples are word2vec and GloVe, recognized for their ability to capture linguistic regularities. In this paper, we demonstrate a very simple, and yet counter-intuitive, postprocessing technique — eliminate the common mean vector and a few top dominating directions from the word vectors — that renders off-the-shelf representations even stronger. The postprocessing is empirically validated on a variety of lexical-level intrinsic tasks (word similarity, concept categorization, word analogy) and sentence-level extrinsic tasks (semantic textual similarity) on multiple datasets and with a variety of representation methods and hyperparameter choices in multiple languages, in each case, the processed representations are consistently better than the original ones. Furthermore, we demonstrate quantitatively in downstream applications that neural network architectures "automatically learn" the postprocessing operation.

**URL:** https://arxiv.org/abs/1702.01417

**Notes:** interesting wistle for the word vectors

### Deep Learning With Denamic Computation Graph

**Authors:** Moshe Looks, Marcello Herreshoff, DeLesley Hutchins & Peter Norvig

**Abstract:** Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library1 of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.

**URL:** https://openreview.net/pdf?id=ryrGawqex

**Notes:** Fresh paper from DeepMind about TF dynamic batching. HOT!

### A Knowledge-Grounded Neural Conversation Model

**Authors:** Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, Michel Galley

**Abstract:** Neural network models are capable of generating extremely natural sounding conversational interactions. Nevertheless, these models have yet to demonstrate that they can incorporate content in the form of factual information or entity-grounded opinion that would enable them to serve in more task-oriented conversational applications. This paper presents a novel, fully data-driven, and knowledge-grounded neural conversation model aimed at producing more contentful responses without slot filling. We generalize the widely-used Seq2Seq approach by conditioning responses on both conversation history and external "facts", allowing the model to be versatile and applicable in an open-domain setting. Our approach yields significant improvements over a competitive Seq2Seq baseline. Human judges found that our outputs are significantly more informative.

**URL:** https://arxiv.org/abs/1702.01932

**Notes:** seq2seq with external facts

### Comparative Study of CNN and RNN for Natural Language Processing

**Authors:** Wenpeng Yin, Katharina Kann, Mo Yu, Hinrich Schütze

**Abstract:** Deep neural networks (DNN) have revolutionized the field of natural language processing (NLP). Convolutional neural network (CNN) and recurrent neural network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting position-invariant features and RNN at modeling units in sequence. The state of the art on many NLP tasks often switches due to the battle between CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection.

**URL:** https://arxiv.org/abs/1702.01923

**Notes:** comparison of RNN and CNN (LeCun vs Schmidthuber) for NLP, neat!

### Neural Discourse Structure for Text Categorization

**Authors:** Yangfeng Ji, Noah Smith

**Abstract:** We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.

**URL:** https://arxiv.org/abs/1702.01829

**Notes:** neural discourse structure

### Living a discrete life in a continuous world: Reference with distributed representations

**Authors:** Gemma Boleda, Sebastian Padó, Nghia The Pham, Marco Baroni

**Abstract:** Reference is the crucial property of language that allows us to connect linguistic expressions to the world. Modeling it requires handling both continuous and discrete aspects of meaning. Data-driven models excel at the former, but struggle with the latter, and the reverse is true for symbolic models. We propose a fully data-driven, end-to-end trainable model that, while operating on continuous multimodal representations, learns to organize them into a discrete-like entity library. We also introduce a referential task to test it, cross-modal tracking. Our model beats standard neural network architectures, but is outperformed by some parametrizations of Memory Networks, another model with external memory.

**URL:** https://arxiv.org/abs/1702.01815

**Notes:** continous representetaions with discrete meanings!

### Beam Search Strategies for Neural Machine Translation

**Authors:** Markus Freitag, Yaser Al-Onaizan

**Abstract:** The basic concept in Neural Machine Translation (NMT) is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is then using a simple left-to-right beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-to- right while keeping a fixed amount of active candidates at each time step. First, this simple search is less adaptive as it also expands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increasing the beam size until no performance improvement can be observed. While you can reach better performance, this has the draw- back of a slower decoding speed. In this paper, we concentrate on speeding up the decoder by applying a more flexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores. We speed up the original decoder by up to 43% for the two language pairs German-English and Chinese-English without losing any translation quality.

**URL:** https://arxiv.org/abs/1702.01806

**Notes:** beam search for NMT

### Ensemble Distillation for Neural Machine Translation

**Authors:** Markus Freitag, Yaser Al-Onaizan, Baskaran Sankaran

**Abstract:** Knowledge distillation describes a method for training a student network to perform better by learning from a stronger teacher network. In this work, we run experiments with different kinds of teacher net- works to enhance the translation performance of a student Neural Machine Translation (NMT) network. We demonstrate techniques based on an ensemble and a best BLEU teacher network. We also show how to benefit from a teacher network that has the same architecture and dimensions of the student network. Further- more, we introduce a data filtering technique based on the dissimilarity between the forward translation (obtained during knowledge distillation) of a given source sentence and its target reference. We use TER to measure dissimilarity. Finally, we show that an ensemble teacher model can significantly reduce the student model size while still getting performance improvements compared to the baseline student network.

**URL:** https://arxiv.org/abs/1702.01802

**Notes:** knowledge distillation for NMT

### Multi-task Coupled Attentions for Category-specific Aspect and Opinion Terms Co-extraction

**Authors:** Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier

**Abstract:** In aspect-based sentiment analysis, most existing methods either focus on aspect/opinion terms extraction or aspect terms categorization. However, each task by itself only provides partial information to end users. To generate more detailed and structured opinion analysis, we propose a finer-grained problem, which we call category-specific aspect and opinion terms extraction. This problem involves the identification of aspect and opinion terms within each sentence, as well as the categorization of the identified terms. To this end, we propose an end-to-end multi-task attention model, where each task corresponds to aspect/opinion terms extraction for a specific category. Our model benefits from exploring the commonalities and relationships among different tasks to address the data sparsity issue. We demonstrate its state-of-the-art performance on three benchmark datasets.

**URL:** https://arxiv.org/abs/1702.01776

**Notes:** multi-task for opinion extraction

### Fast and Accurate Sequence Labeling with Iterated Dilated Convolutions

**Authors:** Emma Strubell, Patrick Verga, David Belanger, Andrew McCallum

**Abstract:** Bi-directional LSTMs have emerged as a standard method for obtaining per-token vector representations serving as input to various token labeling tasks (whether followed by Viterbi prediction or independent classification). This paper proposes an alternative to Bi-LSTMs for this purpose: iterated dilated convolutional neural networks (ID-CNNs), which have better capacity than traditional CNNs for large context and structured prediction. We describe a distinct combination of network structure, parameter sharing and training procedures that is not only more accurate than Bi-LSTM-CRFs, but also 8x faster at test time on long sequences. Moreover, ID-CNNs with independent classification enable a dramatic 14x test-time speedup, while still attaining accuracy comparable to the Bi-LSTM-CRF. We further demonstrate the ability of ID-CNNs to combine evidence over long sequences by demonstrating their improved accuracy on whole-document (rather than per-sentence) inference. Unlike LSTMs whose sequential processing on sentences of length N requires O(N) time even in the face of parallelism, IDCNNs permit fixed-depth convolutions to run in parallel across entire documents. Today when many companies run basic NLP on the entire web and large-volume traffic, faster methods are paramount to saving time and energy costs.

**URL:** https://arxiv.org/abs/1702.02098

**Notes:** seq labeling with dilated CNNs

### Learning similarity preserving representations with neural similarity encoders

**Authors:** Franziska Horn, Klaus-Robert Müller

**Abstract:** Many dimensionality reduction or manifold learning algorithms optimize for retaining the pairwise similarities, distances, or local neighborhoods of data points. Spectral methods like Kernel PCA (kPCA) or isomap achieve this by computing the singular value decomposition (SVD) of some similarity matrix to obtain a low dimensional representation of the original data. However, this is computationally expensive if a lot of training examples are available and, additionally, representations for new (out-of-sample) data points can only be created when the similarities to the original training examples can be computed. We introduce similarity encoders (SimEc), which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly. The model optimizes the same objective as kPCA but in the process it learns a linear or non-linear embedding function (in the form of the tuned neural network), with which the representations of novel data points can be computed - even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings. By creating embeddings for both image and text datasets, we demonstrate that SimEc can, on the one hand, reach the same solution as spectral methods, and, on the other hand, obtain meaningful embeddings from similarities based on human labels.

**URL:** https://arxiv.org/abs/1702.01824

**Notes:** neural PCA

### Semi-Supervised QA with Generative Domain-Adaptive Nets

**Authors:** Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, William W. Cohen

**Abstract:** We study the problem of semi-supervised question answering----utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the Generative Domain-Adaptive Nets. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.

**URL:** https://arxiv.org/abs/1702.02206

**Notes:** Salakhutdinov's fresh article about QA, domain adaptation

### Trainable Greedy Decoding for Neural Machine Translation

**Authors:** Jiatao Gu, Kyunghyun Cho, Victor O.K. Li

**Abstract:** Recent research in neural machine translation has largely focused on two aspects; neural network architectures and end-to-end learning algorithms. The problem of decoding, however, has received relatively little attention from the research community. In this paper, we solely focus on the problem of decoding given a trained neural machine translation model. Instead of trying to build a new decoding algorithm for any specific decoding objective, we propose the idea of trainable decoding algorithm in which we train a decoding algorithm to find a translation that maximizes an arbitrary decoding objective. More specifically, we design an actor that observes and manipulates the hidden state of the neural machine translation decoder and propose to train it using a variant of deterministic policy gradient. We extensively evaluate the proposed algorithm using four language pairs and two decoding objectives and show that we can indeed train a trainable greedy decoder that generates a better translation (in terms of a target decoding objective) with minimal computational overhead.

**URL:** https://arxiv.org/abs/1702.02429

**Notes:** Cho's fresh article, reconding is really the hurting thing

### Automatic Rule Extraction from Long Short Term Memory Networks

**Authors:** W. James Murdoch, Arthur Szlam

**Abstract:** Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.

**URL:** https://arxiv.org/abs/1702.02540

**Notes:** rule extraction from RNN is really interesting - could cost a job for a few linguists

### A Hybrid Convolutional Variational Autoencoder for Text Generation

**Authors:** Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth

**Abstract:** In this paper we explore the effect of architectural choices on learning a Variational Autoencoder (VAE) for text generation. In contrast to the previously introduced VAE model for text where both the encoder and decoder are RNNs, we propose a novel hybrid architecture that blends fully feed-forward convolutional and deconvolutional components with a recurrent language model. Our architecture exhibits several attractive properties such as faster run time and convergence, ability to better handle long sequences and, more importantly, it helps to avoid some of the major difficulties posed by training VAE models on textual data.

**URL:** https://arxiv.org/abs/1702.02390

**Notes:** VAE for texts! with recurrent flavour also

### Iterative Multi-document Neural Attention for Multiple Answer Prediction

**Authors:** Claudio Greco, Alessandro Suglia, Pierpaolo Basile, Gaetano Rossiello, Giovanni Semeraro

**Abstract:** People have information needs of varying complexity, which can be solved by an intelligent agent able to answer questions formulated in a proper way, eventually considering user context and preferences. In a scenario in which the user profile can be considered as a question, intelligent agents able to answer questions can be used to find the most relevant answers for a given user. In this work we propose a novel model based on Artificial Neural Networks to answer questions with multiple answers by exploiting multiple facts retrieved from a knowledge base. The model is evaluated on the factoid Question Answering and top-n recommendation tasks of the bAbI Movie Dialog dataset. After assessing the performance of the model on both tasks, we try to define the long-term goal of a conversational recommender system able to interact using natural language and to support users in their information seeking processes in a personalized way.

**URL:** https://arxiv.org/abs/1702.02367

**Notes:** iterative attention could get more attention

### Question Answering through Transfer Learning from Large Fine-grained Supervision Data

**Authors:** Sewon Min, Minjoon Seo, Hannaneh Hajishirzi

**Abstract:** We show that the task of question answering (QA) can significantly benefit from the transfer learning of models trained on a different large, fine-grained QA dataset. We achieve the state of the art in two well-studied QA datasets, WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique from SQuAD. For WikiQA, our model outperforms the previous best model by more than 8%. We demonstrate that finer supervision provides better guidance for learning lexical and syntactic information than coarser supervision, through quantitative results and visual analysis. We also show that a similar transfer learning procedure achieves the state of the art on an entailment task.

**URL:** https://arxiv.org/abs/1702.02171

**Notes:** transfer learning for QA

### Neural Machine Translation with Source-Side Latent Graph Parsing

**Authors:** Kazuma Hashimoto, Yoshimasa Tsuruoka

**Abstract:** This paper presents a novel neural machine translation model which jointly learns translation and source-side latent graph representations of sentences. Unlike existing pipelined approaches using syntactic parsers, our end-to-end model learns a latent graph parser as part of the encoder of an attention-based neural machine translation model, so the parser is optimized according to the translation objective. Experimental results show that our model significantly outperforms the previous best results on the standard English-to-Japanese translation dataset.

**URL:** https://arxiv.org/abs/1702.02265

**Notes:** NMT on graphs - long time dream, one step to it

### Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization

**Authors:** Ye Zhang, Matthew Lease, Byron C. Wallace

**Abstract:** A fundamental advantage of neural models for NLP is their ability to learn representations from scratch. However, in practice this often means ignoring existing external linguistic resources, e.g., WordNet or domain specific ontologies such as the Unified Medical Language System (UMLS). We propose a general, novel method for exploiting such resources via weight sharing. Prior work on weight sharing in neural networks has considered it largely as a means of model compression. In contrast, we treat weight sharing as a flexible mechanism for incorporating prior knowledge into neural models. We show that this approach consistently yields improved performance on classification tasks compared to baseline strategies that do not exploit weight sharing.

**URL:** https://arxiv.org/abs/1702.02535

**Notes:** weight sharing for categorization could be interesting

### How to evaluate word embeddings? On importance of data efficiency and simple supervised tasks

**Authors:** Stanisław Jastrzebski, Damian Leśniak, Wojciech Marian Czarnecki

**Abstract:** Maybe the single most important goal of representation learning is making subsequent learning faster. Surprisingly, this fact is not well reflected in the way embeddings are evaluated. In addition, recent practice in word embeddings points towards importance of learning specialized representations. We argue that focus of word representation evaluation should reflect those trends and shift towards evaluating what useful information is easily accessible. Specifically, we propose that evaluation should focus on data efficiency and simple supervised tasks, where the amount of available data is varied and scores of a supervised model are reported for each subset (as commonly done in transfer learning). In order to illustrate significance of such analysis, a comprehensive evaluation of selected word embeddings is presented. Proposed approach yields a more complete picture and brings new insight into performance characteristics, for instance information about word similarity or analogy tends to be non--linearly encoded in the embedding space, which questions the cosine-based, unsupervised, evaluation methods. All results and analysis scripts are available online.

**URL:** https://arxiv.org/abs/1702.02170

**Notes:** evaluation of word emdebbings is really should be grounded, for now it is still open, although virtually everyone is using it

### Character-level Deep Conflation for Business Data Analytics

**Authors:** Zhe Gan, P. D. Singh, Ameet Joshi, Xiaodong He, Jianshu Chen, Jianfeng Gao, Li Deng

**Abstract:** Connecting different text attributes associated with the same entity (conflation) is important in business data analytics since it could help merge two different tables in a database to provide a more comprehensive profile of an entity. However, the conflation task is challenging because two text strings that describe the same entity could be quite different from each other for reasons such as misspelling. It is therefore critical to develop a conflation model that is able to truly understand the semantic meaning of the strings and match them at the semantic level. To this end, we develop a character-level deep conflation model that encodes the input text strings from character level into finite dimension feature vectors, which are then used to compute the cosine similarity between the text strings. The model is trained in an end-to-end manner using back propagation and stochastic gradient descent to maximize the likelihood of the correct association. Specifically, we propose two variants of the deep conflation model, based on long-short-term memory (LSTM) recurrent neural network (RNN) and convolutional neural network (CNN), respectively. Both models perform well on a real-world business analytics dataset and significantly outperform the baseline bag-of-character (BoC) model.

**URL:** https://arxiv.org/abs/1702.02640

**Notes:** character-level semantics, another try

### Convolutional Neural Network for Humor Recognition

**Authors:** Lei Chen, Chong MIn Lee

**Abstract:** For the purpose of automatically evaluating speakers' humor usage, we build a presentation corpus containing humorous utterances based on TED talks. Compared to previous data resources supporting humor recognition research, ours has several advantages, including (a) both positive and negative instances coming from a homogeneous data set, (b) containing a large number of speakers, and (c) being open. Focusing on using lexical cues for humor recognition, we systematically compare a newly emerging text classification method based on Convolutional Neural Networks (CNNs) with a well-established conventional method using linguistic knowledge. The CNN method shows its advantages on both higher recognition accuracies and being able to learn essential features automatically.

**URL:** https://arxiv.org/abs/1702.02584

**Notes:** humor recognition is one of research areas of e.g. OpenAI

### Parallel Long Short-Term Memory for Multi-stream Classification

**Authors:** Mohamed Bouaziz, Mohamed Morchid, Richard Dufour, Georges Linarès, Renato De Mori

**Abstract:** Recently, machine learning methods have provided a broad spectrum of original and efficient algorithms based on Deep Neural Networks (DNN) to automatically predict an outcome with respect to a sequence of inputs. Recurrent hidden cells allow these DNN-based models to manage long-term dependencies such as Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM). Nevertheless, these RNNs process a single input stream in one (LSTM) or two (Bidirectional LSTM) directions. But most of the information available nowadays is from multistreams or multimedia documents, and require RNNs to process these information synchronously during the training. This paper presents an original LSTM-based architecture, named Parallel LSTM (PLSTM), that carries out multiple parallel synchronized input sequences in order to predict a common output. The proposed PLSTM method could be used for parallel sequence classification purposes. The PLSTM approach is evaluated on an automatic telecast genre sequences classification task and compared with different state-of-the-art architectures. Results show that the proposed PLSTM method outperforms the baseline n-gram models as well as the state-of-the-art LSTM approach.

**URL:** https://arxiv.org/abs/1702.03402

**Notes:** parallel rnns could be very useful

### Batch Policy Gradient Methods for Improving Neural Conversation Models

**Authors:** Kirthevasan Kandasamy, Yoram Bachrach, Ryota Tomioka, Daniel Tarlow, David Carter

**Abstract:** We study reinforcement learning of chatbots with recurrent neural network architectures when the rewards are noisy and expensive to obtain. For instance, a chatbot used in automated customer service support can be scored by quality assurance agents, but this process can be expensive, time consuming and noisy. Previous reinforcement learning work for natural language processing uses on-policy updates and/or is designed for on-line learning settings. We demonstrate empirically that such strategies are not appropriate for this setting and develop an off-policy batch policy gradient method (BPG). We demonstrate the efficacy of our method via a series of synthetic experiments and an Amazon Mechanical Turk experiment on a restaurant recommendations dataset.

**URL:** https://arxiv.org/abs/1702.03334

**Notes:** offline RL training for chatbots; these guys have invented batch RL approach, very helpful in NLP, also they used importance sampling for more graduate parameter update

### Offline bilingual word vectors, orthogonal transformations and the inverted softmax

**Authors:** Samuel L. Smith, David H. P. Turban, Steven Hamblin, Nils Y. Hammerla

**Abstract:** Usually bilingual word vectors are trained "online". Mikolov et al. showed they can also be found "offline", whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel "inverted softmax" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a "pseudo-dictionary" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.

**URL:** https://arxiv.org/abs/1702.03859

**Notes:** in my experience, we've also encountered the ortho transformations for word vectors

### A Morphology-aware Network for Morphological Disambiguation

**Authors:** Eray Yildiz, Caglar Tirkaz, H. Bahadir Sahin, Mustafa Tolga Eren, Ozan Sonmez

**Abstract:** Agglutinative languages such as Turkish, Finnish and Hungarian require morphological disambiguation before further processing due to the complex morphology of words. A morphological disambiguator is used to select the correct morphological analysis of a word. Morphological disambiguation is important because it generally is one of the first steps of natural language processing and its performance affects subsequent analyses. In this paper, we propose a system that uses deep learning techniques for morphological disambiguation. Many of the state-of-the-art results in computer vision, speech recognition and natural language processing have been obtained through deep learning models. However, applying deep learning techniques to morphologically rich languages is not well studied. In this work, while we focus on Turkish morphological disambiguation we also present results for French and German in order to show that the proposed architecture achieves high accuracy with no language-specific feature engineering or additional resource. In the experiments, we achieve 84.12, 88.35 and 93.78 morphological disambiguation accuracy among the ambiguous words for Turkish, German and French respectively.

**URL:** https://arxiv.org/abs/1702.03654

**Notes:** should have a look for the corpora they used, I have my work on that

### Learning to Parse and Translate Improves Neural Machine Translation

**Authors:** Akiko Eriguchi, Yoshimasa Tsuruoka, Kyunghyun Cho

**Abstract:** There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RG.

**URL:** https://arxiv.org/abs/1702.03525

**Notes:** Cho's fresh article about NMT, grammar here is additional net

### Exploring loss function topology with cyclical learning rates

**Authors:** Leslie N. Smith, Nicholay Topin

**Abstract:** We present observations and discussion of previously unreported phenomena discovered while training residual networks. The goal of this work is to better understand the nature of neural networks through the examination of these new empirical results. These behaviors were identified through the application of Cyclical Learning Rates (CLR) and linear network interpolation. Among these behaviors are counterintuitive increases and decreases in training loss and instances of rapid training. For example, we demonstrate how CLR can produce greater testing accuracy than traditional training despite using large learning rates. Files to replicate these results are available at this [URL](https://github.com/lnsmith54/exploring-loss)

**URL:** https://arxiv.org/abs/1702.04283

**Notes:** I've heard, that could allow us to improve results *after* the training is over, intriguing

### Frustratingly Short Attention Spans in Neural Language Modeling

**Authors:** Michał Daniluk, Tim Rocktäschel, Johannes Welbl, Sebastian Riedel

**Abstract:** Neural language models predict the next token using a latent representation of the immediate token history. Recently, various methods for augmenting neural language models with an attention mechanism over a differentiable memory have been proposed. For predicting the next token, these models query information from a memory of the recent history which can facilitate learning mid- and long-range dependencies. However, conventional attention mechanisms used in memory-augmented neural language models produce a single output vector per time step. This vector is used both for predicting the next token as well as for the key and value of a differentiable memory of a token history. In this paper, we propose a neural language model with a key-value attention mechanism that outputs separate representations for the key and value of a differentiable memory, as well as for encoding the next-word distribution. This model outperforms existing memory-augmented neural language models on two corpora. Yet, we found that our method mainly utilizes a memory of the five most recent output representations. This led to the unexpected main finding that a much simpler model based only on the concatenation of recent output representations from previous time steps is on par with more sophisticated memory-augmented neural language models.

**URL:** https://arxiv.org/abs/1702.04521

**Notes:** attention is said to be a sot of memory, and short memory is surprise for language modeling

### A Dependency-Based Neural Reordering Model for Statistical Machine Translation

**Authors:** Christian Hadiwinoto, Hwee Tou Ng

**Abstract:** In machine translation (MT) that involves translating between two languages with significant differences in word order, determining the correct word order of translated words is a major challenge. The dependency parse tree of a source sentence can help to determine the correct word order of the translated words. In this paper, we present a novel reordering approach utilizing a neural network and dependency-based embeddings to predict whether the translations of two source words linked by a dependency relation should remain in the same order or should be swapped in the translated sentence. Experiments on Chinese-to-English translation show that our approach yields a statistically significant improvement of 0.57 BLEU point on benchmark NIST test sets, compared to our prior state-of-the-art statistical MT system that uses sparse dependency-based reordering features.

**URL:** https://arxiv.org/abs/1702.04510

**Notes:** dependency-based approach for NMT

### Training Language Models Using Target-Propagation

**Authors:** Sam Wiseman, Sumit Chopra, Marc'Aurelio Ranzato, Arthur Szlam, Ruoyu Sun, Soumith Chintala, Nicolas Vasilache

**Abstract:** While Truncated Back-Propagation through Time (BPTT) is the most popular approach to training Recurrent Neural Networks (RNNs), it suffers from being inherently sequential (making parallelization difficult) and from truncating gradient flow between distant time-steps. We investigate whether Target Propagation (TPROP) style approaches can address these shortcomings. Unfortunately, extensive experiments suggest that TPROP generally underperforms BPTT, and we end with an analysis of this phenomenon, and suggestions for future work.

**URL:** https://arxiv.org/abs/1702.04770

**Notes:** the people tried to reinvent backprop for the RNNs and failed; praise them for sharing this experience

### Latent Variable Dialogue Models and their Diversity

**Authors:** Kris Cao, Stephen Clark

**Abstract:** We present a dialogue generation model that directly captures the variability in possible responses to a given input, which reduces the `boring output' issue of deterministic dialogue models. Experiments show that our model generates more diverse outputs than baseline models, and also generates more consistently acceptable output than sampling from a deterministic encoder-decoder model.

**URL:** https://arxiv.org/abs/1702.05962

**Notes:** hidden variable adaptation to dialog generation

### Reproducing and learning new algebraic operations on word embeddings using genetic programming

**Authors:** Roberto Santana

**Abstract:** Word-vector representations associate a high dimensional real-vector to every word from a corpus. Recently, neural-network based methods have been proposed for learning this representation from large corpora. This type of word-to-vector embedding is able to keep, in the learned vector space, some of the syntactic and semantic relationships present in the original word corpus. This, in turn, serves to address different types of language classification tasks by doing algebraic operations defined on the vectors. The general practice is to assume that the semantic relationships between the words can be inferred by the application of a-priori specified algebraic operations. Our general goal in this paper is to show that it is possible to learn methods for word composition in semantic spaces. Instead of expressing the compositional method as an algebraic operation, we will encode it as a program, which can be linear, nonlinear, or involve more intricate expressions. More remarkably, this program will be evolved from a set of initial random programs by means of genetic programming (GP). We show that our method is able to reproduce the same behavior as human-designed algebraic operators. Using a word analogy task as benchmark, we also show that GP-generated programs are able to obtain accuracy values above those produced by the commonly used human-designed rule for algebraic manipulation of word vectors. Finally, we show the robustness of our approach by executing the evolved programs on the word2vec GoogleNews vectors, learned over 3 billion running words, and assessing their accuracy in the same word analogy task.

**URL:** https://arxiv.org/abs/1702.05624

**Notes:** the genetic algorithms for word embeddings inquiry that's a fresh view: the author creates the algorithm with basic operations on word embeddings

### soc2seq: Social Embedding meets Conversation Model

**Authors:** Parminder Bhatia, Marsal Gavalda, Arash Einolghozati

**Abstract:** While liking or upvoting a post on a mobile app is easy to do, replying with a written note is much more difficult, due to both the cognitive load of coming up with a meaningful response as well as the mechanics of entering the text. Here we present a novel textual reply generation model that goes beyond the current auto-reply and predictive text entry models by taking into account the content preferences of the user, the idiosyncrasies of their conversational style, and even the structure of their social graph. Specifically, we have developed two types of models for personalized user interactions: a content-based conversation model, which makes use of location together with user information, and a social-graph-based conversation model, which combines content-based conversation models with social graphs.

**URL:** https://arxiv.org/abs/1702.05512

**Notes:** promo-article of Yak-Yak guys, pretty interesting: they combined persona-based style, location, social graph embeddings to produce possible answers for social networks

### An Attention-Based Deep Net for Learning to Rank

**Authors:** Baiyang Wang, Diego Klabjan

**Abstract:** In information retrieval, learning to rank constructs a machine-based ranking model which given a query, sorts the search results by their degree of relevance or importance to the query. Neural networks have been successfully applied to this problem, and in this paper, we propose an attention-based deep neural network which better incorporates different embeddings of the queries and search results with an attention-based mechanism. This model also applies a decoder mechanism to learn the ranks of the search results in a listwise fashion. The embeddings are trained with convolutional neural networks or the word2vec model. We demonstrate the performance of this model with image retrieval and text querying data sets.

**URL:** https://arxiv.org/abs/1702.06106

**Notes:** attention applied to LtR NNs

### Collaborative Deep Reinforcement Learning

**Authors:** Kaixiang Lin, Shu Wang, Jiayu Zhou

**Abstract:** Besides independent learning, human learning process is highly improved by summarizing what has been learned, communicating it with peers, and subsequently fusing knowledge from different sources to assist the current learning goal. This collaborative learning procedure ensures that the knowledge is shared, continuously refined, and concluded from different perspectives to construct a more profound understanding. The idea of knowledge transfer has led to many advances in machine learning and data mining, but significant challenges remain, especially when it comes to reinforcement learning, heterogeneous model structures, and different learning tasks. Motivated by human collaborative learning, in this paper we propose a collaborative deep reinforcement learning (CDRL) framework that performs adaptive knowledge transfer among heterogeneous learning agents. Specifically, the proposed CDRL conducts a novel deep knowledge distillation method to address the heterogeneity among different learning tasks with a deep alignment network. Furthermore, we present an efficient collaborative Asynchronous Advantage Actor-Critic (cA3C) algorithm to incorporate deep knowledge distillation into the online training of agents, and demonstrate the effectiveness of the CDRL framework using extensive empirical evaluation on OpenAI gym.

**URL:** https://arxiv.org/abs/1702.05796

**Notes:** guys're using knowledge distillation for collaboration; also really strange: it seems that this paper is in proceedings of ACL Woodstock 1997 conference, which is very unlikely to me

### Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning

**Authors:** Sahil Sharma, Aravind S. Lakshminarayanan, Balaraman Ravindran

**Abstract:** Reinforcement Learning algorithms can learn complex behavioral patterns for sequential decision making tasks wherein an agent interacts with an environment and acquires feedback in the form of rewards sampled from it. Traditionally, such algorithms make decisions, i.e., select actions to execute, at every single time step of the agent-environment interactions. In this paper, we propose a novel framework, Fine Grained Action Repetition (FiGAR), which enables the agent to decide the action as well as the time scale of repeating it. FiGAR can be used for improving any Deep Reinforcement Learning algorithm which maintains an explicit policy estimate by enabling temporal abstractions in the action space. We empirically demonstrate the efficacy of our framework by showing performance improvements on top of three policy search algorithms in different domains: Asynchronous Advantage Actor Critic in the Atari 2600 domain, Trust Region Policy Optimization in Mujoco domain and Deep Deterministic Policy Gradients in the TORCS car racing domain.

**URL:** https://arxiv.org/abs/1702.06054

**Notes:** action repetition on almost every SotA algo in RL

### Collaborative Deep Reinforcement Learning for Joint Object Search

**Authors:** Xiangyu Kong, Bo Xin, Yizhou Wang, Gang Hua

**Abstract:** We examine the problem of joint top-down active search of multiple objects under interaction, e.g., person riding a bicycle, cups held by the table, etc.. Such objects under interaction often can provide contextual cues to each other to facilitate more efficient search. By treating each detector as an agent, we present the first collaborative multi-agent deep reinforcement learning algorithm to learn the optimal policy for joint active object localization, which effectively exploits such beneficial contextual information. We learn inter-agent communication through cross connections with gates between the Q-networks, which is facilitated by a novel multi-agent deep Q-learning algorithm with joint exploitation sampling. We verify our proposed method on multiple object detection benchmarks. Not only does our model help to improve the performance of state-of-the-art active localization models, it also reveals interesting co-detection patterns that are intuitively interpretable.

**URL:** https://arxiv.org/abs/1702.05573

**Notes:** gated cross-connections between networks with different experience, some kind of knowledge distillation in my opinion

### Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks

**Authors:** Luo Chunjie, Zhan jianfeng, Wang lei, Yang Qiang

**Abstract:** Traditionally, multi-layer neural networks use dot product between the output vector of previous layer and the incoming weight vector as the input to activation function. The result of dot product is unbounded, thus causes large variance. Large variance makes the model sensitive to the change of input distribution, thus results in bad generalization and aggravates the internal covariate shift. To bound dot product, we propose to use cosine similarity instead of dot product in neural network, which we call cosine normalization. Experiments show that cosine normalization in fully connected neural networks can reduce the test err with lower divergence compared with other normalization techniques. Applied to convolutional networks, cosine normalization also significantly enhances the accuracy of classification.

**URL:** https://arxiv.org/abs/1702.05870

**Notes:** these guys are saying that cosine normalization is better (in a way) than batch-norm - on MNIST, but that could be useful knowledge

### On Loss Functions for Deep Neural Networks in Classification

**Authors:** Katarzyna Janocha, Wojciech Marian Czarnecki

**Abstract:** Deep neural networks are currently among the most commonly used classifiers. Despite easily achieving very good performance, one of the best selling points of these models is their modular design - one can conveniently adapt their architecture to specific needs, change connectivity patterns, attach specialised layers, experiment with a large amount of activation functions, normalisation schemes and many others. While one can find impressively wide spread of various configurations of almost every aspect of the deep nets, one element is, in authors' opinion, underrepresented - while solving classification problems, vast majority of papers and applications simply use log loss. In this paper we try to investigate how particular choices of loss functions affect deep models and their learning dynamics, as well as resulting classifiers robustness to various effects. We perform experiments on classical datasets, as well as provide some additional, theoretical insights into the problem. In particular we show that L1 and L2 losses are, quite surprisingly, justified classification objectives for deep nets, by providing probabilistic interpretation in terms of expected misclassification. We also introduce two losses which are not typically used as deep nets objectives and show that they are viable alternatives to the existing ones.

**URL:** https://arxiv.org/abs/1702.05659

**Notes:** most widely-used losses comparison, very nice

### Dataset Augmentation in Feature Space

**Authors:** Terrance DeVries, Graham W. Taylor

**Abstract:** Dataset augmentation, the practice of applying a wide array of domain-specific transformations to synthetically expand a training set, is a standard tool in supervised learning. While effective in tasks such as visual recognition, the set of transformations must be carefully designed, implemented, and tested for every new domain, limiting its re-use and generality. In this paper, we adopt a simpler, domain-agnostic approach to dataset augmentation. We start with existing data points and apply simple transformations such as adding noise, interpolating, or extrapolating between them. Our main insight is to perform the transformation not in input space, but in a learned feature space. A re-kindling of interest in unsupervised representation learning makes this technique timely and more effective. It is a simple proposal, but to-date one that has not been tested empirically. Working in the space of context vectors generated by sequence-to-sequence models, we demonstrate a technique that is effective for both static and sequential data.

**URL:** https://arxiv.org/abs/1702.05538

**Notes:** augmentation in feature space - interesting, could be used when we have already trained net (say, AlexNet) and relatively few data

### An Extended Framework for Marginalized Domain Adaptation

**Authors:** Gabriela Csurka, Boris Chidlovski, Stephane Clinchant, Sophia Michel

**Abstract:** We propose an extended framework for marginalized domain adaptation, aimed at addressing unsupervised, supervised and semi-supervised scenarios. We argue that the denoising principle should be extended to explicitly promote domain-invariant features as well as help the classification task. Therefore we propose to jointly learn the data auto-encoders and the target classifiers. First, in order to make the denoised features domain-invariant, we propose a domain regularization that may be either a domain prediction loss or a maximum mean discrepancy between the source and target data. The noise marginalization in this case is reduced to solving the linear matrix system AX=B which has a closed-form solution. Second, in order to help the classification, we include a class regularization term. Adding this component reduces the learning problem to solving a Sylvester linear matrix equation AX+BX=C, for which an efficient iterative procedure exists as well. We did an extensive study to assess how these regularization terms improve the baseline performance in the three domain adaptation scenarios and present experimental results on two image and one text benchmark datasets, conventionally used for validating domain adaptation methods. We report our findings and comparison with state-of-the-art methods.

**URL:** https://arxiv.org/abs/1702.05993

**Notes:** additional loss for transfer learning, some improvement are actually shown

### Revisiting Perceptron: Efficient and Label-Optimal Active Learning of Halfspaces

**Authors:** Songbai Yan, Chicheng Zhang

**Abstract:** It has been a long-standing problem to efficiently learn a linear separator using as few labels as possible. In this work, we propose an efficient perceptron-based algorithm for actively learning homogeneous linear separators under uniform distribution. Under bounded noise, where each label is flipped with probability at most η, our algorithm achieves near-optimal $O~(frac{d}{(1−2η)^2}log(frac{1}{ϵ}))$ label complexity in time $O~(frac{d^2}{ϵ(1−2η)^2})$, and significantly improves over the best known result (Awasthi et al., 2016). Under adversarial noise, where at most ν fraction of labels can be flipped, our algorithm achieves near-optimal $O~(log(frac{1}{ϵ}))$ label complexity in time $O~(frac{d^2}{ϵ})$, which is better than the best known label complexity and time complexity in Awasthi et al. (2014).

**URL:** https://arxiv.org/abs/1702.05581

**Notes:** active learning of perceptron, so 50-s and so 2000-s at the same time

### Distributed Second-Order Optimization Using Kronecker-Factored Approximations

**Authors:** Jimmy Ba, Roger Grosse, James Martens

**Abstract:** As more computational resources become available, machine learning researchers train ever larger neural networks on millions of data points using stochastic gradient descent (SGD). Although SGD scales well in terms of both the size of dataset and the number of parameters of the model, it has rapidly diminishing returns as parallel computing resources increase. Second-order optimization methods have an affinity for well-estimated gradients and large mini-batches, and can therefore benefit much more from parallel computation in principle. Unfortunately, they often employ severe approximations to the curvature matrix in order to scale to large models with millions of parameters, limiting their effectiveness in practice versus well-tuned SGD with momentum. The recently proposed K-FAC method (Martens and Grosse, 2015) uses a stronger and more sophisticated curvature approximation, and has been shown to make much more per-iteration progress than SGD, while only introducing a modest overhead. In this paper, we develop a version of K-FAC that distributes the computation of gradients and additional quantities required by K-FAC across multiple machines, thereby taking advantage of the method’s superior scaling to large mini-batches and mitigating its additional overheads. We provide a Tensorflow implementation of our approach which is easy to use and can be applied to many existing codebases without modification. Additionally, we develop several algorithmic enhancements to K-FAC which can improve its computational performance for very large models. Finally, we show that our distributed K-FAC method speeds up training of various state-of-the-art ImageNet classification models by a factor of two compared to an improved form of Batch Normalization (Ioffe and Szegedy, 2015).

**URL:** https://jimmylba.github.io/papers/nsync.pdf

**Notes:** Optimisation which makes nets 50% faster, wow. The second-orders pastly haven't been used due to complexity and seeming slowness.

### Mimicking Ensemble Learning with Deep Branched Networks

**Authors:** Byungju Kim, Youngsoo Kim, Yeakang Lee, Junmo Kim

**Abstract:** This paper proposes a branched residual network for image classification. It is known that high-level features of deep neural network are more representative than lower-level features. By sharing the low-level features, the network can allocate more memory to high-level features. The upper layers of our proposed network are branched, so that it mimics the ensemble learning. By mimicking ensemble learning with single network, we have achieved better performance on ImageNet classification task.

**URL:** https://arxiv.org/abs/1702.06376

**Notes:** really short paper from KAIST, about branching in resnets

### Style Transfer Generative Adversarial Networks: Learning to Play Chess Differently

**Authors:** Muthuraman Chidambaram, Yanjun Qi

**Abstract:** The idea of style transfer has largely only been explored in image-based tasks, which we attribute in part to the specific nature of loss functions used for style transfer. We propose a general formulation of style transfer as an extension of generative adversarial networks, by using a discriminator to regularize a generator with an otherwise separate loss function. We apply our approach to the task of learning to play chess in the style of a specific player, and present empirical evidence for the viability of our approach.

**URL:** https://arxiv.org/abs/1702.06762

**Notes:** next step in style transfer - playing chess, the salt in the loss and nothing more

### Learning to Draw Dynamic Agent Goals with Generative Adversarial Networks

**Authors:** Shariq Iqbal, John Pearson

**Abstract:** We address the problem of designing artificial agents capable of reproducing human behavior in a competitive game involving dynamic control. Given data consisting of multiple realizations of inputs generated by pairs of interacting players, we model each agent's actions as governed by a time-varying latent goal state coupled to a control model. These goals, in turn, are described as stochastic processes evolving according to player-specific value functions depending on the current state of the game. We model these value functions using generative adversarial networks (GANs) and show that our GAN-based approach succeeds in producing sample gameplay that captures the rich dynamics of human agents. The latent goal dynamics inferred and generated by our model has applications to fields like neuroscience and animal behavior, where the underlying value functions themselves are of theoretical interest.

**URL:** https://arxiv.org/abs/1702.07319

**Notes:** GAN here takes place of RL, I think that could lead us to simultenous ude of GAN and RL in tasks

### Data Distillation for Controlling Specificity in Dialogue Generation

**Authors:** Jiwei Li, Will Monroe, Dan Jurafsky

**Abstract:** People speak at different levels of specificity in different situations. Depending on their knowledge, interlocutors, mood, etc.} A conversational agent should have this ability and know when to be specific and when to be general. We propose an approach that gives a neural network--based conversational agent this ability. Our approach involves alternating between \emph{data distillation} and model training : removing training examples that are closest to the responses most commonly produced by the model trained from the last round and then retrain the model on the remaining dataset. Dialogue generation models trained with different degrees of data distillation manifest different levels of specificity. We then train a reinforcement learning system for selecting among this pool of generation models, to choose the best level of specificity for a given input. Compared to the original generative model trained without distillation, the proposed system is capable of generating more interesting and higher-quality responses, in addition to appropriately adjusting specificity depending on the context. Our research constitutes a specific case of a broader approach involving training multiple subsystems from a single dataset distinguished by differences in a specific property one wishes to model. We show that from such a set of subsystems, one can use reinforcement learning to build a system that tailors its output to different input contexts at test time.

**URL:** https://arxiv.org/abs/1702.06703

**Notes:** Jurafsky et al. are adding more divensification to dialog-agent traning; this could be interpreted as an active learning in a way

### Context-Aware Prediction of Derivational Word-forms

**Authors:** Ekaterina Vylomova, Ryan Cotterell, Timothy Baldwin, Trevor Cohn

**Abstract:** Derivational morphology is a fundamental and complex characteristic of language. In this paper we propose the new task of predicting the derivational form of a given base-form lemma that is appropriate for a given context. We present an encoder--decoder style neural network to produce a derived form character-by-character, based on its corresponding character-level representation of the base form and the context. We demonstrate that our model is able to generate valid context-sensitive derivations from known base forms, but is less accurate under a lexicon agnostic setting.

**URL:** https://arxiv.org/abs/1702.06675

**Notes:** Katya you're the best! Seriously, interesting problem is solved with char-level model

### Tackling Error Propagation through Reinforcement Learning: A Case of Greedy Dependency Parsing

**Authors:** Minh Le, Antske Fokkens

**Abstract:** Error propagation is a common problem in NLP. Reinforcement learning explores erroneous states during training and can therefore be more robust when mistakes are made early in a process. In this paper, we apply reinforcement learning to greedy dependency parsing which is known to suffer from error propagation. Reinforcement learning improves accuracy of both labeled and unlabeled dependencies of the Stanford Neural Dependency Parser, a high performance greedy parser, while maintaining its efficiency. We investigate the portion of errors which are the result of error propagation and confirm that reinforcement learning reduces the occurrence of error propagation.

**URL:** https://arxiv.org/abs/1702.06794

**Notes:** RL applied to dependency parsing

### Active One-shot Learning

**Authors:** Mark Woodward, Chelsea Finn

**Abstract:** Recent advances in one-shot learning have produced models that can learn from a handful of labeled examples, for passive classification and regression tasks. This paper combines reinforcement learning with one-shot learning, allowing the model to decide, during classification, which examples are worth labeling. We introduce a classification task in which a stream of images are presented and, on each time step, a decision must be made to either predict a label or pay to receive the correct label. We present a recurrent neural network based action-value function, and demonstrate its ability to learn how and when to request labels. Through the choice of reward function, the model can achieve a higher prediction accuracy than a similar model on a purely supervised task, or trade prediction accuracy for fewer label requests.

**URL:** https://arxiv.org/abs/1702.06559

**Notes:** RL approach (action-value) applied to active learning

